{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b28f481",
   "metadata": {},
   "source": [
    "### One Hot encording\n",
    "\n",
    "One-hot encoding is a simple technique used in text analysis to convert words into numerical form so deep learning models can process them. Each unique word in a vocabulary is represented as a vector of zeros with a single one indicating the word’s position. This makes words easy for models to recognize mathematically, but it has important limitations. The vectors are very large and sparse, which increases memory and computation costs. More importantly, one-hot encoding does not capture meaning or relationships between words, so similar words appear completely unrelated. Because of these limitations, it is often replaced by word embeddings in modern deep learning systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d71e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encording(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    vocabulary = sorted(set(words))\n",
    "    word_to_index = {word: i for i,\n",
    "        word in enumerate(vocabulary)}\n",
    "    one_hot_matrix = np.zeros((\n",
    "        len(words), len(vocabulary)), dtype=int)\n",
    "    for i, word in enumerate(words):\n",
    "        one_hot_matrix[i, word_to_index[word]] =1\n",
    "    return one_hot_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292e18e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18e3054d",
   "metadata": {},
   "source": [
    "This function takes a sentence, converts it to lowercase, and splits it into individual words, then builds a vocabulary of unique words and assigns each word an index. It creates a matrix where each row represents a word in the sentence and each column represents a vocabulary term. For each word, the function places a 1 in the column corresponding to that word’s index and 0s elsewhere, producing a one-hot encoded representation. It returns both the resulting matrix and the vocabulary, making the text usable for basic deep learning or text analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a84a451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'do', 'go', 'or', 'pizzeria', 'prefer', 'restaurant?', 'should', 'to', 'we', 'you']\n",
      "One_hot_encording_matrix:\n",
      " [[0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Should we go to a pizzeria or do you prefer a restaurant?\"\n",
    "one_hot_matrix, vocabulary = one_hot_encording(sentence)\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"One_hot_encording_matrix:\\n\", one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b446fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'about', 'dining?', 'do', 'go', 'hotel,', 'just', 'normal', 'or', 'pizzeria', 'prefer', 'restaurant?,', 'should', 'to', 'we', 'what', 'you']\n",
      "One_hot_encording_matrix:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Should we go to a pizzeria or do you prefer a restaurant?, what about a hotel, or just a normal dining?\"\n",
    "one_hot_matrix, vocabulary = one_hot_encording(sentence)\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"One_hot_encording_matrix:\\n\", one_hot_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bdfe1",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "\t1.\tSimplicity – It is very easy to implement and understand.\n",
    "\t2.\tDeterministic representation – Each word has a unique, unambiguous vector.\n",
    "\t3.\tNo assumptions – Doesn’t rely on prior knowledge about word meanings.\n",
    "\t4.\tGood for small vocabularies – Works efficiently when the dataset is small.\n",
    "\t5.\tCompatibility – Can be used as a straightforward input for classical machine learning models or basic neural networks.\n",
    "\n",
    "In short, one-hot encoding is simple, clear, and deterministic, making it a good starting point for learning text representation despite its scalability limitations.\n",
    "\n",
    "#### Downside\n",
    "\n",
    "The main downside of one-hot encoding is that it produces very large, sparse vectors as the vocabulary grows, which increases memory usage and computation cost. It also fails to capture any semantic meaning or relationships between words, so similar words (like good and great) are treated as completely unrelated. Additionally, one-hot encoding cannot handle unseen words well and does not scale efficiently for large text datasets, making it impractical for most modern deep learning applications compared to word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00db1b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
