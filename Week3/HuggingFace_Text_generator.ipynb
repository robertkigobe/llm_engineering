{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c63fea8",
   "metadata": {},
   "source": [
    "\t### Text Gwnerators\n",
    "\tThis code loads a pretrained GPT-2 causal language model, runs it on Apple Silicon using MPS, and generates a short continuation of text from a given prompt using Hugging Face’s high-level pipeline API.\n",
    "    •\ttorch: Loads PyTorch, the deep learning framework that runs the model.\n",
    "\t•\tpipeline: A high-level abstraction that bundles tokenization → model inference → decoding into one call.\n",
    "\t•\tAutoTokenizer: Automatically loads the correct tokenizer for the chosen model.\n",
    "\t•\tAutoModelForCausalLM: Loads a causal language model (predicts the next token based only on previous tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e289b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c240932a961a46f8b8bcfd89edd93970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8051ac5f4755452ea4ec023d9e43ddc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642ab483d9744b6ab0ab6fbea89e8df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c8a5b5e12b474dbf9cbb3080230847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79c9ca61ce8482c87c3a5824d53b485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cd50b8d2f74edda74bb0f9558e8c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed273fb568e4c9797ad5eba94a83879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, I did not think that my father was in love with any of his relatives and that he was in love with a man who had only one child.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id =\"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(\"mps\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"mps\")\n",
    "result = generator(\"Once upon a time,\", max_new_tokens=30)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8233c29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the chicken cross the road, and why did they not stop the car at the bridge and give chase?\n",
      "\n",
      "I've always been fascinated with the idea of the mysterious, but\n"
     ]
    }
   ],
   "source": [
    "result = generator(\"Why did the chicken cross the road,\", max_new_tokens=30)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652e09ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me some nice words to a girl, and I'll give you some real words.\"\n",
      "\n",
      "\"Why? Why don't you go back to your room, and I'll teach you some words?\"\n",
      "\n",
      "\"I'm sorry, but I'll be back with you soon.\" He sighed.\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"You're going to take a shower, too, Miss Granger.\" He said.\n",
      "\n",
      "\"You know, you'll get out of this place in a week or two. I'm afraid\n"
     ]
    }
   ],
   "source": [
    "result = generator(\"Give me some nice words to a girl,\", max_new_tokens=100)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f76278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
